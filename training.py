# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1avTWwCZ7pgBGqrfrMm3wRYwZ5lIrMO6H
"""

pip install --upgrade pip

pip install mediapipe-model-maker

from google.colab import files
import os
import tensorflow as tf
assert tf.__version__.startswith('2')

from mediapipe_model_maker import gesture_recognizer

import matplotlib.pyplot as plt

!wget https://universe.roboflow.com/ds/gE2TzpqR3J?key=PteZiresb6

# Commented out IPython magic to ensure Python compatibility.
# %cd gestures

!unzip /content/gestures/gE2TzpqR3J?key=PteZiresb6

# Commented out IPython magic to ensure Python compatibility.
# %cd ..

dataset_path = "/content/gestures/train"

print(dataset_path)
labels = []
for i in os.listdir(dataset_path):
  if os.path.isdir(os.path.join(dataset_path, i)):
    labels.append(i)
print(labels)

import os
import matplotlib.pyplot as plt

NUM_EXAMPLES = 5
# Assuming 'dataset_path' and 'labels' are defined elsewhere in your code
# For demonstration, let's mock them:
# dataset_path = 'path/to/your/dataset'
# labels = ['label1', 'label2', 'label3']

for label in labels:
    label_dir = os.path.join(dataset_path, label)
    # Ensure there are files in the directory before trying to list them
    if os.listdir(label_dir):
        example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]
        fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))
        for i in range(NUM_EXAMPLES):
            # Check if there are enough examples to plot
            if i < len(example_filenames):
                axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))
                axs[i].get_xaxis().set_visible(False)
                axs[i].get_yaxis().set_visible(False)
            else:
                # Hide empty subplots if there are fewer than NUM_EXAMPLES
                fig.delaxes(axs[i])
        fig.suptitle(f'Showing {min(NUM_EXAMPLES, len(example_filenames))} examples for {label}')
        plt.show() # Display the current figure
        plt.close(fig) # Close the figure immediately after displaying it
    else:
        print(f"No images found in {label_dir}")

data = gesture_recognizer.Dataset.from_folder(
    dirname=dataset_path,
    hparams=gesture_recognizer.HandDataPreprocessingParams()
)
train_data, rest_data = data.split(0.8)
validation_data, test_data = rest_data.split(0.5)

from mediapipe_model_maker import gesture_recognizer

# Hyperparameters based on 9000 image dataset
hparams = gesture_recognizer.HParams(
    learning_rate=0.001,  # Significantly increased learning rate
    epochs=500,           # More epochs might be needed with a higher LR
    batch_size=16,        # Increased batch size for more stable gradients
    # steps_per_epoch=... # Remove this line if Model Maker handles it automatically
                          # based on dataset size and batch size.
                          # If you must set it, ensure it's train_data_size / batch_size
    shuffle=True,
    lr_decay=0.99,        # Slower learning rate decay
    gamma=0,              # Start with 0 if dataset is balanced, or 1 if mildly imbalanced
    export_dir="exported_model"
)

# Increased model capacity by doubling the layer widths
model_options = gesture_recognizer.ModelOptions(
    dropout_rate=0.1,
    layer_widths=[1024, 512, 256] # Keep these for now
)

options = gesture_recognizer.GestureRecognizerOptions(
    model_options=model_options,
    hparams=hparams
)

model = gesture_recognizer.GestureRecognizer.create(
    train_data=train_data,
    validation_data=validation_data,
    options=options
)

print("Model training configuration updated with increased layer widths and revised hparams.")

loss, acc = model.evaluate(test_data, batch_size=1)
print(f"Test loss:{loss}, Test accuracy:{acc}")

model.export_model()
!ls exported_model

files.download('exported_model/gesture_recognizer.task')

files.download('/content/exported_model')





